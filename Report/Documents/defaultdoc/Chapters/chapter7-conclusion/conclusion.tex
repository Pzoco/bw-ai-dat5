In this chapter of the report we summarize all the work done, and in what measure our goals were fulfilled. \\

For this project we wanted to create an intelligent bot that plays Starcraft Broodwar. With this idea in mind, we constructed two different intelligent characteristics. The first one is the making of decisions during a real-time combat. It takes this decisions using reinforcement learning Q approximation and potential fields. The second one is the prediction of build orders and analysis of threat level. We accomplish this task by using Bayesian networks. \\

So in these two cases the bot can act intelligently, but how intelligent is it compared to a human player? \\

The Bayesian networks that are implemented are very simple, which means that the bot will only be able to recognize a small set of build orders. Even if these networks were made more complex, there will always be build orders which will not get recognized. So enabling the bot to recognize the most known build orders seems sufficient. The Reinforcement learning and Potential fields merge preformed as expected. The bot can control the units more efficiently than the best Starcraft Broodwar player from our group, and it learns continuously to better achieve the goal of killing all the enemy units. \\

Another of out goals was to effectively apply machine intelligence theories in the modelling of our bot, which goes hand in hand with the third goal we defined in the problem statement, create a bot that improves by playing Starcraft Broodwar. This goal was achieved with the Reinforcement learning merge with Potential fields. We started defining the potential fields' vectors for modelling the movement of our units and then using this completed product to create the Q-function of our Reinforcement Learning. By doing this merge we managed to create a simple, linear, and comprehensive function that models the combat environment in Starcraft Broodwar. With this function the Reinforcement learning algorithm had all the elements needed to learn and improve. After several training sessions our bot manages to increase the number of kills and almost converge to the true values for the Q-function and Potential.\\

The Bayesian networks predictions are not improved by playing. All the probabilities for the networks are obtained form replays with professional Starcraft players. The numbers might be unrealistic, but as we showed in the test of these networks, it was able to predict the right build order and threat level. But it would definitely be improved using data mining from several match replays. \\

We also described that we wanted to make a full bot, completely able to play a normal game against another player. And in theory, the bot is able to do this. But for it to preform well, better techniques and more precise information is needed in the Strategy Manager and the placing of buildings. The Strategy Manager can not control the other managers as we intended completely. The Reinforcement learning and potential fields only work well for the training maps right now and not for the normal maps, because it has not been trained to move in environments where there are buildings and other units that can block the path. Nevertheless, if the managers controlled the game enough to only activate the reinforcement learning and potential fields when needed, the bot could be used in real games.

Finally,  


