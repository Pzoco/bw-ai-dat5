%here goes the final converging test of the A2G9 - only the text ofc! - and ref to the picture at F8-11
\section{Convergence Analysis}

As mentioned before, we considered a lower $\alpha$ value and a higher $\gamma$ value would deliver better results in the training for the Reinforcement learning agent. Therefore we prioritized the training of two particular combinations of them: $\alpha= 0.4$ - $\gamma = 0.6$ and $\alpha= 0.2$ - $\gamma = 0.9$. We ran a couple of hundred thousand game iterations in each one of these training values. The results obtained from this training show the different trends the weights follow throughout the entire learning process. We considered it important to analyze this behaviour to show the training results for our agent. \\


\subsection*{Convergence for $\alpha= 0.2$ and $\gamma = 0.9$}

By convergence we mean that the agent has found the perfect values that work every time. In other words, the agent learns to get the highest reward. In figure \ref{fig:app_a2g9_test} one can clearly see that the agent tries with different values and after the almost 250.000 iterations it's beginning to narrow them down to a more stable line instead of trying higher values. The blue line which says \textit{edge} has a drop way below zero, and that might be a buffer overflow, but if we only look at the \textit{ally} and \textit{squad} we can see that the agent learned that sticking together in a group is better than attacking marines individually. The \textit{cooldown} is beginning to have a more stable line which means that the agent learned that after firing, its good to flee to avoid getting injured. The \textit{maximum distance} is not as high as we expected, since the higher it is the more aggressive the vultures are. Even more iterations could perfect these numbers to a convergence and the vultures would then win every game. 


