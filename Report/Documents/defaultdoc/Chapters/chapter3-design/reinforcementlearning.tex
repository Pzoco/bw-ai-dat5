\section{Agent Learning}\label{agent_learning}
This section will explain some of the different reinforcement learning techniques;  each one explained briefly and analyzed to evaluate its relevance. 

\subsection*{Reinforcement Learning}
Reinforcement learning (RL) is a method used to build models or functions that learn from experiences and examples. The basic idea is that for every action in an environment, there is a reward or some feedback that reinforces all actions that have a bigger reward. In a larger scale, the task of reinforcement learning in this report is the process to discover the optimal path; the series of actions that accomplish the best possible total reward at the end of the process. This reward depends entirely on the agent's policy, better defined as the strategy it follows for accompishing something. 

There are different RL techniques that depend on the amount of information we have available for learning. They can be classified into passive and active RL methods. \textit{In passive RL the agents policy is fixed and the task is to learn the utilities of each state.} \cite[p764]{rl} This implies that the environment is fully observable and the agent knows the future impact of its actions. Then the learning part of the algorithm is only in charge of learning the best strategy for the already defined probabilities. \textit{The active reinforcement learning does not have a fixed policy to begin with, and the agent must decide what actions to take} \cite[p771]{rl}. So the agent basically explores considering that it can't look ahead for more than a move or predict the effects of its actions in the future.

\subsubsection{Markov Desicion Process} \label{mdp}

With the intent of further expanding on the RL capabilities, we need a way to define the task of the agent. A general formulation of the problem starts based on Markov Decision Processes: \textit{In a Markov Decision Process (MDP) the agent can perceive a set $S$ of distinct states of its environment and has a set $A$ of actions that it can perform. At each discrete time step $t$, the agent senses the current state $s_t$, chooses a current action $a_t$, and performs it. The environment responds by giving the agent a reward $r_t = r(s_t,a_t)$ and by producing the succeeding state $s_{t+l}= \delta (s_t,a_t)$. Here the functions $\delta$ and $r$ are part of the environment and are not necessarily known to the agent. In an MDP, the functions $\delta (s_t,a_t)$ and $r(s_t,a_t)$ depend only on the current state and action, and not on earlier states or actions.}\cite[p370]{ml_tom_mitchel} 

With this we almost have enough information to build the problem structure of the agent. Then we need to consider a function that describes the  \emph{total cummulative reward} of a set of actions. It could be any function: dicounted cummulative reward ($\sum^{\infty}_{i=0}\gamma^ir_{t+i}$), average reward($\lim_{h\to\infty}\frac{1}{h}\sum^{h}_{i=0}r_{t+i}$), finite horizon reward ($\sum^{h}_{i=0}r_{t+i}$). This reward function %!!!!!!!!!!!!!

And then, we can define the agent's learning task. In passive RL it is to find a \emph{utility function} or how good a certain policy is. In active RL is to find a policy that maximizes this reward function's value, in other words, finding the \emph{optimal policy}.\cite{ml_tom_mitchel}

%Need to define y and explain cummulative reward, average reward and finite horizon.

\subsubsection{Direct Utility Estimation \& Bellman Rules}

In passive RL, the method of Direct Utility Estimation (DUE) follows the idea that the utility of a state is the expected total reward from that state into the future. Then, at the end of a trial, the RL algorithm for DUE will trace back through all the observed rewards and calculate the estimated utility for every state. It basically a process of inductive learning that observes a set of data that is completely known. The formula for calculating the utility values follows the Bellman equations for a fixed policy \cite{rl}:\\

$U^\pi(s) = R(s)+\gamma\sum_{s'}T(s,\pi(s),s') U^\pi(s')$\\

The problem with this technique is that it ignores the relationship between the states (they are not independent form each other), and it only learns at the end of a trial, therefore missing several oportunities for learning and converging very slowly\cite{rl}.

\subsubsection{Adaptive Dynamic Programming}

Searching for a way of considering the constraints between states, we come accross Adaptive Dynamic Programming. This is an agent that learns the transition model for an enviroment while solving the MDP. In a fully observable enviroment, this means that you use a transition model and the observed rewards into the Bellman equations to calculate the utilities of a state. In simpler terms, it means that while learning each step (state-action pair) you keep track of the outcome and then save it into a table of probabilities. At the end you have a comprehensive probability table for all the transitions, in this way you know a reliable model for knowing what is going to happen with every state-action pair\cite{rl}.

The problem is it is impossible to calculate for large state spaces, specially if you have to run several trials to exhaust every resonable possiblity for every transition. Therefore not usable for our agent. 

\subsubsection{Temporal Difference Learning}

The combination of the previous two methods, \textit{using the observed transitions to adjust the values of the observed states so that they agree with the constrait equations}\cite[p767]{rl}, produces the following rule (to be applied every time a transition occurs form state $s$ to $s'$):\\

$U^\pi(s) = U^\pi(s) + \alpha(R(s)+\gamma U^\pi(s') -  U^\pi(s))$\\

This update rule (temporal-difference equation) uses the difference in utilities between succesive states. It shifts or updates the estimates towards the ideal equation. There are several things to notice here. The first is that since it updates with the next state ($s'$) it might seem like it adapts too much to every trial, but in reality this update rule is applied several times, therefore producing an average and isolating rare cases. \cite{rl} 

The second is the appearance of $\alpha$, also known as the learning rate parameter; how much it learns from an specific trial. Normaly the value of  $\alpha$ would be something like: $\frac{1}{1+numberOfVisits(s,a)}$ \cite[p382]{ml_tom_mitchel}. Which then decreases the magnitude of the update proportionally to how many times you visited a state-action pair. 

\subsubsection{Q Learning}

All the previous methods have been explained considering there is a fixed policy that determines the behavior of the agent. Now the task shifts to active reinforcement learning because we need our agent to decide which action to take in each state-action pair. We need and active temporal difference learner (that learns the utility function $U$). In Q learning this utility function is basically:s\cite{rl}

\begin{center}
$U(s) = max_a(Q(s,a))$
\end{center}

Where $Q(s,a)$ represents the value of making and action $a$ in a state $s$. And if we use the temporal difference approach for Q Learning we have the following updating rule:\cite{rl} 

\begin{center}
$Q(a,s) \leftarrow  Q(a,s)  + \alpha [R(s) + \gamma max_{a'}(Q(s',a')) - Q(a,s)]$
\end{center}

Q learning is a temporal difference learner that \textit{does not need a model for either learning or action selection}\cite[p775]{rl}. It simply executes the updating rule every time and action $a$ is executed in a state $s$ that leads to an state $s'$. The only restriction we have left is that the algorithm for exploration for this Q-learning temporal difference agent is the same as in the ADP agent \cite[p776]{rl} . That means that it keeps track of every movement and saves the statistics in a table. It easily learns and saves the \emph{optimal policy} for small state spaces, but it is impossible to keep track for larger ones. 

\section{Generalization of Q-Learning}	\label{qlearning}

The choice of using reinforcement learning was based on the necessity to train our AI to improve after every match or test. Since our environment covers a lot of different factors and variables, we decided to use a form of active reinforcement learning that simplifies the complexity and size of all the different states. 

Generalization in reinforcement learning takes into consideration huge state spaces by representing them as function approximations. This function reduces the complexity of mapping all the states considerably and allows the learning agent to generalize from the visited states to the non-visited ones. \textit{This function is viewed as approximate because it might not be the case that the true utility or Q-function can be represented in the chosen form} \cite[p777]{rl}. 

\subsection{Q-Learning Functions}

The starcraft environment has several factors that could be considered important for defining a moment in time; units, time, enemies, distances, map elements, etc. Since formulating a model considering all the factors made the function too complex, we focused on the factors that could model the environment as closely as possible without increasing the size and computability time of each calculation. 

\textit{There is the problem that there could fail to be any function in the chosen hypothesis space that approximates the true utility function sufficiently well. As in all inductive learning there is a trade-off between the size of the hypothesis space and the time it takes to learn the function. A larger hypothesis space increases the likelihood that a good approximation can be found, but also means the convergence is likely to be delayed.}\cite[p778]{rl}

We combine the generalization of Q-learning with the potential fields to obtain a reasonable model of the starcraft environment. We do this by transforming all the potential fields (per unit) into a simplified version of the Q function used by the agent. The first thing we needed to specify was a representation of all the data relevant for a specific time or frame, our hypothesis space. 

\subsubsection{State - Hypothesis Space}

We define a state in our environment as a combination of the most important factors that interact with the agent and the game. Basically, it consists of all the distances used in the potential fields plus the numbers required to calculate a comprehensive reward function. \\
\begin{displaymath}
                       State = \begin{cases}
                         da \\  dua \\  ds \\  dsv \\ de \\ due \\ dc \\ duc \\ wr \\ sr \\ numberOfUnits \\ healthLost \\ damageDealt \\ numberOfKills \\ time
                      \end{cases}
\end{displaymath}\\

The variables numberOfUnits, healthLost, damageDealt, numberOfKills and time are variables accessible through the entire game, therefore used as part of our reward function. The description the rest of the factors or distances is the same as mentioned before in the potential field's documentation (MISSING REFERENCE!!!!!!!!!!!!!). 

\subsubsection{Function Approximation}

After defining a state in the game we created a linear function approximation to ensure convergence of each value. The function basically takes all the forces that determine the magnitude of the potential field vectors as coefficients or weights in the $\hat{Q}$ function. \\ 

$\hat{Q}_f = f_{MDP} (2de - due) + f_{AU} (2da - dua) + f_{EAC} (2dc - duc) + f_{S}  (2ds - dsv) + f_{CD} (2de - due)$ \\ 

This function is not a thorough model of the true utility function, but it covers all the factors that affect the movement of a unit. Since every unit is controlled independently with this movement model/function, it covers the purpose of using the computer's capabilities of controlling each unit's movement separately and optimally (micro).

Notice that there are forces that are dependant on the exact same variables, like Cooldown and Maximum Distance Positioning. This coefficients vary in magnitude because they are updated and calculated under different circumstances and throughout different moments in the game. This is furthered explained in the implementation documentation (REFERENCE!!!!). 

%explain f.au and f.eac and ref it to PF.

\subsubsection{Updating Rules}

We use the updating rule or Q-learning equation that evolves from Widrow-Huff rule or Delta Rule reducing the temporal difference between successive states:  \cite{rl} \\ 
 
$\Theta_i \leftarrow \Theta_i + \alpha [ R(s) + \gamma(max\hat{Q}_\Theta(a',s'))-\hat{Q}_\Theta(a,s) ] \frac{\partial \hat{Q}_\Theta(a,s)}{\partial\Theta_i}$ \\ 

\begin{flushleft}
Where $\Theta_i$ is each one of the coefficients in the Q-approximation. In the context of our  $\hat{Q}_f$ function, it would represent the forces: $f_{MDP}$, $f_{AU}$, $f_{EAC}$, $f_{S}$, $f_{CD}$. 
\end{flushleft} 

\begin{flushleft}
$\alpha$  - Is the learning rate, as mentioned before it means how much you modify the value of the coefficient $\theta_i$ to fit the current example or situation, it learns from each visit to every state. Its a number, $0 < \alpha < 1$, that in a normal temporal difference equation (LINK TO TDE!!!!!) would (optimaly) decrease according to how many times a state is visited \cite{rl}. Since every one of our states is visited inifinitely many times, the value $\alpha$ can be a fixed number that we modify manually. The higher the value, the more you learn from every specific case.
\end{flushleft} 

\begin{flushleft}
$\gamma$  - Is the discount factor, this determines the importance of the future rewards. Its a number, $0 < \gamma < 1$, the closest $\gamma$ gets to 1 the more it takes into account future rewards. A $\gamma$ value close to zero would maximize the immediate rewards. 
\end{flushleft} 

\begin{flushleft}
$R(S)$ -  represents the reward function for the current state.
\end{flushleft} 

\begin{flushleft}
$\hat{Q}_\Theta(a,s)$ - Is the value of the $\hat{Q}$ function for the next position (one step ahead). The next position depends on the current state $s$ and the action $a$ performed by the agent from that state.
\end{flushleft} 

\begin{flushleft}
$max(\hat{Q}_\Theta(a',s'))$ - Is the highest possible $\hat{Q}$ value calculated from the next position (two steps ahead). The highest possible option (for every $a'$) once the current state $s$ has performed an action $a$ and is in a new state $s' = \delta(a,s)$. 
\end{flushleft} 

\begin{flushleft}
$\frac{\partial \hat{Q}_\Theta(a,s)}{\partial\Theta_i}$  - Is the partial derivative of the $\hat{Q}$ function with respect to the current $\Theta_i$, in other words the variables or factors that afect only that coefficient $\Theta_i$. And in the case of our  linear $\hat{Q}$ function, always a constant representing some distances in a certain state. 
\end{flushleft} 

\begin{flushleft}
So we are left with the following updating rules for each of the coefficients/forces:
\end{flushleft} 

\begin{flushleft}
Maximum Distance Positioning 
$f_{MDP}  \leftarrow f_{MDP}  + \alpha [ R(s) + \gamma (max(\hat{Q}_{f} (a',s')))-\hat{Q}_{f} (a,s) ](2de - due)$
\end{flushleft} 

\begin{flushleft}
Ally Units  \\ 
$f_{AU}  \leftarrow f_{AU} + \alpha [ R(s) + \gamma (max(\hat{Q}_{f} (a',s')))-\hat{Q}_{f} (a,s) ](2da - dua)$ 
\end{flushleft} 

\begin{flushleft}
Edges and Cliffs
$f_{EAC}  \leftarrow f_{EAC}  + \alpha [ R(s) + \gamma (max(\hat{Q}_{f} (a',s')))-\hat{Q}_{f} (a,s) ] (2dc - duc)$ 
\end{flushleft} 

\begin{flushleft}
Squad  \\ 
$f_{S}  \leftarrow f_{S}  + \alpha [ R(s) + \gamma (max(\hat{Q}_{f} (a',s')))-\hat{Q}_{f} (a,s) ] (2ds - dsv)$
\end{flushleft} 

\begin{flushleft}
Cooldown  \\ 
$f_{CD}  \leftarrow f_{CD}  + \alpha [ R(s) + \gamma (max(\hat{Q}_{f} (a',s')))-\hat{Q}_{f} (a,s) ](2de - due)$ 
\end{flushleft} 

% \ref{mdp}
These updating rules should eventually converge to values that are very close to the optimal Q function, considering all the restrictions for convergence. In a normal Q-learning process the rules are the following: \textit{First, we must assume the system is a deterministic markov desicion process. Second, we must assume the immediate reward values are bounded; that is, there exists some positive constant $c$ such that for all states $s$ and actions $a$, $r(s,a) < c$. Third, we assume the agent selects actions in such a fashion that it visits every possible state-action pair infinitely often.} \cite[p377]{ml_tom_mitchel} 

The only difference between this process and our aproximation to Q-learning is the generalization of unvisited states. But this generalization is also guaranteed: \textit{These update rules can be shown to converge to the closest possible approximation to the true function when the function approximator is linear in the parameters. }\cite[p779]{rl} Then, the final result should be the perfect magnitud for the potential fields to guide every unit's movement.

%explain Widrow-Huff rule and Delta Rule. maybe Q-learning equation, if it is not explained before.
% Talk about the elements i the equetion, why do it look like this? what effect do the elements have(why we go one step into the futur). 

\subsubsection{Reward Function}

We created a reward function that takes into consideration all the factors to grade the performance of the agent. The reward function gives positive points for keeping the highest number of units alive, negative points for loosing health, positive points for both killing or damaging the enemies, and a negative reward for every time frame in the game that goes by. This way we ensure that the agent wants to attack the enemy while protecting its units; but not prioritizing protecting the units. We control that the agent chooses attacking over hiding or running away by making the reward proportional to how short the match is. \\ 

$R(s) = C_1 numberOfUnits  -  C_2 healthLost  +   C_3 damageDealt  +   C_4 numberOfKills -  C_5 time$ \\ 

We ensure that the reward complies with the convergence restrictions for Q learning(LINK TO CONVERGENCE RESTRICTIONS!!!). The reward function is bound, $R(s) <= C$. The upper bound of the constant C defined by $C = C_1 startingNumberOfUnits  +   C_3 maximumDamageDealt  +   C_4 maximumNumberOfKills$. The lower bound defined by $C = -  C_2 maximumHealthLost -  C_5 maximumTime$, and $maximumTime$ being the number of time frames for a manually defined limit (20-30 seconds). 

\subsection{Q-Learning Conclusions}