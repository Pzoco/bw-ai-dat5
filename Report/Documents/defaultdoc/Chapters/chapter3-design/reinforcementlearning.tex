\section{Agent Learning}\label{agent_learning}
%introduce the two and mention we choosed RL
This section addresses various different learning techniques and methods that could be used in the AI agent. There will be a brief explanation of the the two options we considered. First, the neural network section will contain a basic overview of how it works, followed by the examination of its usability in our proyect. Then the reinforecement learning section will come across some of the different reinforcement learning techniques;  each one explained briefly and analized to evaluate its relevance. 

\subsection*{Neural Networks}			 
			


\subsection*{Reinforcement Learning}

Reinforcement learning  [RL] is a method used to build models or functions that learn from experiences and examples. The basic idea is that for every action in an environment, there is a reward or some feedback that reinforces all actions that have a bigger reward. In a larger scale, the task of reinforcement learning is the process to discover the optimal path; the series of actions that accomplish the best possible total reward at the end of the process. This reward depends entirely on the agent's policy, better defined as the strategy it follows for accompishing something. 

% used algorithm for the lack of a better word...
There are diferent RL techniques that depend on the amout of information we have avaliable for learning. They can be cassified into passive and active RL methods. In passive RL the agents policy is fixed and the task is to learn the utilities of each state \cite[p764]{rl}. This implies that the enviroment is fully observable and the agent knows the future impact of its actions. Then the learning part of the algorithm is only in charge of learning the best strategy for the already defined probabilities. The active reinforcement learning does not have a fixed policy to begin with, and the agent must decide what actions to take \cite[p771]{rl}. So the agent basicaly explores considering that it can't look ahead for more than a move or predict the efects of its actions in the future. 

%Paragraph about how active reinforcement learning can grow exponentialy in complexity and space -> therefore DP

\section{Generalization of Q-Learning}	\label{qlearning}

The choice of using reinforcement learning was based on the necessity to train our AI to improve after every match or test. Since our environment covers a lot of different factors and variables, we decided to use a form of active reinforcement learning that simplifies the complexity and size of all the different states. 

Generalization in reinforcement learning takes into consideration huge state spaces by representing them as function approximations. This function reduces the complexity of mapping all the states considerably and allows the learning agent to generalize from the visited states to the non-visited ones. This function is viewed as aproximate because it might not be the case that the true utility or Q-function can be represented in the chosen form \cite[p777]{rl}. 

\subsection{Q-Learning functions}

The starcraft enviroment has several factors that could be considered important for defining a moment in time; units, time, enemies, distances, map elements, etc. Since formulating a model considering all the factors made the function too complex, we focused on the factors that could model the enviroment as closely as possible without incresing the size and computability time of each calculation. 

We combine the generalization of Q-learning with the potential fields to obtain a resonable model of the starcraft enviroment. We do this by transforming all the potential fields (per unit) into a simplified version of the Q function used by the agent. The first thing we needed to specify was a representation of all the data relevant for a specific time or frame in the game. We define a state in our enviroment as a combination of the most important factors that interact with the agent and the game. Basically, it consists of all the distances used in the potential fields plus the numbers required to calculate a comprehensive reward function. 

\begin{displaymath}
                       State = \begin{cases}
                         da \\  dua \\  ds \\  dsv \\ de \\ due \\ dc \\ duc \\ wr \\ sr \\ numberOfUnits \\ healthLost \\ damageDealt \\ numberOfKills
                      \end{cases}
\end{displaymath}

The description of every one of the distances is mentioned before in the potential field's documentation (MISSING REFERENCE!!!!!!!!!!!!!). 

After defining a state in the game we created a linear function that 



The function basically takes all the forces that determine the magnitude of the potential field vectors as coefficients or weights in the Q function. 

$Q = f_{MDP} (2de - due)+f_{AU} (2da - dua+f_{EAC} (2dc - duc)+f_{S}  (2ds - dsv)+f_{CD} (2de - due)$





