\section{Reinforcement Learning}

	Reinforcement learning is a method used to build a model or functions that learn from experience and examples. The basic idea is that for every action in an environment there is a reward or feedback that reinforces all the actions that have bigger rewards. In a bigger scale the task of reinforcement learning is the process to discover the optimal path or series of actions to accomplish the best possible reward at the end of the process. 

	The choice of using reinforcement learning was based on the necessity to train our AI to perform better after every match or test. Since our environment covers a lot of different factors and variables, we then decided on using a form of active reinforcement learning that simplifies the complexity and size of all the different states in the environment: Generalization. 

	Generalization in reinforcement learning takes into consideration huge state spaces by representing them by function approximation. This function reduces the complexity of mapping all the states considerably and allows the learning agent to generalize form the visited states to the not visited ones. 

	We combine the reinforcement learning method with the potential fields by transforming all the potential fields of each unit into the representation of the utility function used by the agent. We do this by talking all the forces that determine the magnitude of the potential field vectors as coefficients in the utility function:
	
	
	Reinforcement learning can be used in two different ways, either an active or a passive way. By using the passive reinforcement learning one defines a specific policy of the agent, and the task is to learn the utilities of states \cite{rl}. The basic goal is to learn how good the predefined policy is, and it does that by iterative going through the states without knowing the rewards beforehand and by trying and 'remembering' the best way. The active reinforcement learning does not have a fixed policy to begin with, and the agent must decide what actions to take \cite[p771]{rl} by that in mind we have to learn an agent a complete model with what outcome for every action. The basic idea behind active reinforcement learning is that in the end it should be taught and obeying the optimal policy. It learns the utility function 

	% There should be something about:
		% Passive Reinforcement / Active Reinforcement
			% Direct utility estimation
			% Adaptive dynamic programming
			% Temporal difference learning
			% Exploration
			% Q-Learning
			
%write about NEURAL NETWORK, why we don't use it.





\subsection{Neural Network}			 
			

%Intro
Neural network [NN] can be used to teach an agent to behave a certain way.
A neural network is similar to how the brain obtains new information, and learns it. The human brain is made up of about 100 billion neurons, and each neuron is connected with thousand of other neurons communicating via electrochemical signals \cite{nn}. The neuron gets a lot of input and when the neuron has filled a threshold it fires through something called an axon. This automatic method of filling up information and storing it when a certain threshold is reached can be implemented in a computer.



%Description
There are different ways of connection a neural network, but we will be concentrating on the method called \textit{feedforward}. The way to feed the neuron is by applying different inputs to it, and before entering the neuron the input goes through some weights, that is adjusted manually. Just like before if the input in total is greater or equal to a certain threshold the neuron will output a signal, if it's less it will output zero.
%Technical details
A neuron can have from 1 to n number of inputs, and each input will be multiplied by its weight
If we have input $x_1, x_2, x_3, x_n$ the weights calculated will look like $x_1 w_1 + x_2 w_2 + x_3 w_3 + .. +  x_n w_n$

%Why we're not using NN

We have chosen not to use neural network and the main reason is that neural network is very slow, and in a real-time-strategy game where we can have around 50.000 states, the computation time will be very slow and the input will be way to high every minute it has to compute approximately 50.000 states. A generalization function would be a better approach because it approximates the states, and doesn't have to compute the entire state-space in a game. 
%50.000 states because APM Multiplied with TimeSpent in the game 
