\subsection{Neural Network}			 
			
%Intro
Neural network [NN] can be used to teach an agent to behave a certain way.
A neural network is similar to how the brain obtains new information, and learns it. The human brain is made up of about 100 billion neurons, and each neuron is connected with thousand of other neurons communicating via electrochemical signals \cite{nn}. The neuron gets a lot of input and when the neuron has filled a threshold it fires through something called an axon. This automatic method of filling up information and storing it when a certain threshold is reached can be implemented in a computer.

%Description
There are different ways of connection a neural network, but we will be concentrating on the method called \textit{feedforward}. The way to feed the neuron is by applying different inputs to it, and before entering the neuron the input goes through some weights, that is adjusted manually. Just like before if the input in total is greater or equal to a certain threshold the neuron will output a signal, if it's less it will output zero.
%Technical details
A neuron can have from 1 to n number of inputs, and each input will be multiplied by its weight
If we have input $x_1, x_2, x_3, x_n$ the weights calculated will look like $x_1 w_1 + x_2 w_2 + x_3 w_3 + .. +  x_n w_n$

%Why we're not using NN

We have chosen not to use neural network and the main reason is that neural network is very slow, and in a real-time-strategy game where we can have around 50.000 states, the computation time will be very slow and the input will be way to high every minute it has to compute approximately 50.000 states. A generalization function would be a better approach because it approximates the states, and doesn't have to compute the entire state-space in a game. 
%50.000 states because APM Multiplied with TimeSpent in the game 


\section{Reinforcement Learning}

	Reinforcement learning is a method used to build a model or functions that learn from experience and examples. The basic idea is that for every action in an environment there is a reward or feedback that reinforces all the actions that have bigger rewards. In a bigger scale the task of reinforcement learning is the process to discover the optimal path or series of actions to accomplish the best possible reward at the end of the process. 

	The choice of using reinforcement learning was based on the necessity to train our AI to perform better after every match or test. Since our environment covers a lot of different factors and variables, we then decided on using a form of active reinforcement learning that simplifies the complexity and size of all the different states in the environment: 

	Generalization in reinforcement learning takes into consideration huge state spaces by representing them by function approximation. This function reduces the complexity of mapping all the states considerably and allows the learning agent to generalize form the visited states to the not visited ones. We combine the reinforcement learning method with the potential fields by transforming all the potential fields of each unit into the representation of the utility function used by the agent. We do this by taking all the forces that determine the magnitude of the potential field vectors as coefficients in the utility function.
	
	
	Reinforcement learning can be used in two different ways, either an active or a passive way. By using the passive reinforcement learning one defines a specific policy of the agent, and the task is to learn the utilities of states \cite{rl}. The basic goal is to learn how good the predefined policy is, and it does that by iterative going through the states without knowing the rewards beforehand and by trying and 'remembering' the best way. The active reinforcement learning does not have a fixed policy to begin with, and the agent must decide what actions to take \cite[p771]{rl} by that in mind we have to learn an agent a complete model with what outcome for every action. The basic idea behind active reinforcement learning is that in the end it should be taught and obeying the optimal policy.

	% There should be something about:
		% Passive Reinforcement / Active Reinforcement
			% Direct utility estimation
			% Adaptive dynamic programming
			% Temporal difference learning
			% Exploration
			% Q-Learning - Explaning now
			
\subsection{Direct utility estimation}
\subsection{Adaptive dynamic programming}
\subsection{Temporal difference learning}
\subsection{Exploration}
\subsection{Q-Learning}			
Just to give an example of how large state spaces easily can get we can take a look at some well known games chess and backgammon. They are a tiny subset of the real world and their state spaces contain on the order of $10^50$ and $10^120$ states, and if the agent should visit all these states to learn how to play the game it will run i an absurd long time \cite[p. 777]{rl}. That's why we are going to use function approximation, which basicly means that we don't show the function as a table.

$U_\Theta(s) = \Theta_1 f_1(s) + \Theta_2 f_2(s) + ... + \Theta_n f_n(s) $

The parameters $\Theta = \Theta_1, ..., \Theta_n$ so the evaluation function above $U_\Theta$ approximates the utility function. Instead of the explanation above with the state space of $10^120$ values in a table.

The approximation looks like:
%NEEDS AN ACCENT DAN!!!
$\Theta_i \leftarrow \Theta_i + \alpha [ R(s) + \gamma maxQ_\Theta(a',s')-Q_\Theta(a,s) ] \frac{d}{d\Theta_i}(Q_\Theta(a,s))$


and using our variables:\\
First we define what a state can contain:
$s = {da, ds, dsv, de, due, wv, dc, dua, numberOfUnits, healthLost, damageDone, fallenEnemies}$
The description of these variables can be read here \ref{cha3_variables}
$R(s) = C_1 numberOfUnits - C_2 healthLost +  C_3 damageGiven +  C_4 numberOfFallenEnemies$\\

$Q = f_{MDP1} (2de - due)-f_{AN}(da)-f_{WCD}(2de-due)-f_{EAC}(de)+f_s(ds-dsv)-f_{MDP2}(2de-due)$





%write about NEURAL NETWORK, why we don't use it.

We are going to use approximation of Q-Learning, so it can handle a huge state space



