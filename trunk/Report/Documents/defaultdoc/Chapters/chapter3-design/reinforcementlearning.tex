\section{Agent Learning}\label{agent_learning}
%introduce the two and mention we choosed RL
This section addresses various different learning techniques and methods that could be used in the AI agent. There will be a brief explanation of the the two options we considered. First, the neural network section will contain a basic overview of how it works, followed by the examination of its usability in our proyect. Then the reinforecement learning section will come across some of the different reinforcement learning techniques;  each one explained briefly and analized to evaluate its relevance. 

\subsection*{Neural Networks}			 
			


\subsection*{Reinforcement Learning}

Reinforcement learning  [RL] is a method used to build models or functions that learn from experiences and examples. The basic idea is that for every action in an environment, there is a reward or some feedback that reinforces all actions that have a bigger reward. In a larger scale, the task of reinforcement learning is the process to discover the optimal path; the series of actions that accomplish the best possible total reward at the end of the process. This reward depends entirely on the agent's policy, better defined as the strategy it follows for accompishing something. 

% used algorithm for the lack of a better word...
There are diferent RL techniques that depend on the amout of information we have avaliable for learning. They can be cassified into passive and active RL methods. In passive RL the agents policy is fixed and the task is to learn the utilities of each state \cite[p764]{rl}. This implies that the enviroment is fully observable and the agent knows the future impact of its actions. Then the learning part of the algorithm is only in charge of learning the best strategy for the already defined probabilities. The active reinforcement learning does not have a fixed policy to begin with, and the agent must decide what actions to take \cite[p771]{rl}. So the agent basicaly explores considering that it can't look ahead for more than a move or predict the efects of its actions in the future. 

% Normal Q learning

%Paragraph about how active reinforcement learning can grow exponentialy in complexity and space -> therefore DP

%you are missing a lot about temporal difference learners 

\section{Generalization of Q-Learning}	\label{qlearning}

The choice of using reinforcement learning was based on the necessity to train our AI to improve after every match or test. Since our environment covers a lot of different factors and variables, we decided to use a form of active reinforcement learning that simplifies the complexity and size of all the different states. 

Generalization in reinforcement learning takes into consideration huge state spaces by representing them as function approximations. This function reduces the complexity of mapping all the states considerably and allows the learning agent to generalize from the visited states to the non-visited ones. This function is viewed as aproximate because it might not be the case that the true utility or Q-function can be represented in the chosen form \cite[p777]{rl}. 

\subsection{Q-Learning functions}

The starcraft enviroment has several factors that could be considered important for defining a moment in time; units, time, enemies, distances, map elements, etc. Since formulating a model considering all the factors made the function too complex, we focused on the factors that could model the enviroment as closely as possible without incresing the size and computability time of each calculation. 

\textit{There is the problem that there could fail to be any function in the chosen hypothesis space that approximates the true utility function sufficiently well. As in all inductive learning there is a trade-off betewwn the size of te hypothesis space and the time it takes to learn the function. A larger hypothesis space increases the likelihood that a good aproximation can be found, but also means the convergence is likely to be delayed.}\cite[p778]{rl}

We combine the generalization of Q-learning with the potential fields to obtain a resonable model of the starcraft enviroment. We do this by transforming all the potential fields (per unit) into a simplified version of the Q function used by the agent. The first thing we needed to specify was a representation of all the data relevant for a specific time or frame, our hypothesis space. 

\subsubsection{State - Hypothesis Space}

We define a state in our enviroment as a combination of the most important factors that interact with the agent and the game. Basically, it consists of all the distances used in the potential fields plus the numbers required to calculate a comprehensive reward function. 

\\
\begin{displaymath}
                       State = \begin{cases}
                         da \\  dua \\  ds \\  dsv \\ de \\ due \\ dc \\ duc \\ wr \\ sr \\ numberOfUnits \\ healthLost \\ damageDealt \\ numberOfKills \\ time
                      \end{cases}
\end{displaymath}

The variables numberOfUnits, healthLost, damageDealt, numberOfKills and time are variables accessible through the entire game, therefore used as part of our reward function. The description the rest of the factors or distances is the same as mentioned before in the potential field's documentation (MISSING REFERENCE!!!!!!!!!!!!!). 

\subsubsection{Function Approximation}

After defining a state in the game we created a linear function approximation to ensure convergence of each value. The function basically takes all the forces that determine the magnitude of the potential field vectors as coefficients or weights in the \hat{Q} function. \\ 

$\hat{Q}_f = f_{MDP} (2de - due) + f_{AU} (2da - dua) + f_{EAC} (2dc - duc) + f_{S}  (2ds - dsv) + f_{CD} (2de - due)$ \\ 

This function is not a thorough model of the true utility function, but it covers all the factors that affect the movement of a unit. Since every unit is controled independently with this movement model/function, it covers the purpose of using the computer's capabilities of controling each unit's movement separately and optimaly (micro).

Notice that there are forces that are dependant on the exact same variables, like Cooldown and Maximum Distance Positioning. This coefficients vary in magnitude because they are updated and calculated under different circumstances and througout different moments in the game. This is furthered explained in the implementation decumentation (REFERENCE!!!!). 

\subsubsection{Updating Rules}

We use the updating rule or Q-learning equation that evolves from Widrow-Huff rule or Delta Rule reducing the temporal difference between successive states:  \cite[p779]{rl} \\ 
 
$\Theta_i \leftarrow \Theta_i + \alpha [ R(s) + \gamma(max\hat{Q}_\Theta(a',s'))-\hat{Q}_\Theta(a,s) ] \frac{\partial \hat{Q}_\Theta(a,s)}{\partial\Theta_i}$ \\ 

%explain each clause of the formula

\begin{flushleft}
So we are left with 5 updating rules for each of the coefficients:
\end{flushleft} 

\begin{flushleft}
Maximum Distance Positioning 
$f_{MDP}  \leftarrow f_{MDP}  + \alpha [ R(s) + \gamma (max(\hat{Q}_f_{MDP} (a',s')))-\hat{Q}_f_{MDP} (a,s) ](2de - due)$
\end{flushleft} 

\begin{flushleft}
Ally Units  \\ 
$f_{AU}  \leftarrow f_{AU} + \alpha [ R(s) + \gamma (max(\hat{Q}_f_{AU} (a',s')))-\hat{Q}_f_{AU} (a,s) ](2da - dua)$ 
\end{flushleft} 

\begin{flushleft}
Edges and Cliffs
$f_{EAC}  \leftarrow f_{EAC}  + \alpha [ R(s) + \gamma (max(\hat{Q}_f_{EAC} (a',s')))-\hat{Q}_f_{EAC} (a,s) ] (2dc - duc)$ 
\end{flushleft} 

\begin{flushleft}
Squad  \\ 
$f_{S}  \leftarrow f_{S}  + \alpha [ R(s) + \gamma (max(\hat{Q}_f_{S} (a',s')))-\hat{Q}_f_{S} (a,s) ] (2ds - dsv)$
\end{flushleft} 

\begin{flushleft}
Cooldown  \\ 
$f_{CD}  \leftarrow f_{CD}  + \alpha [ R(s) + \gamma (max(\hat{Q}_f_{CD} (a',s')))-\hat{Q}_f_{CD} (a,s) ](2de - due)$ 
\end{flushleft} 


%convergence guarantees

\textit{These update rules can be shown to converge to the closest possible approximation to the true function when the function approximator is linear in the parameters. }\cite[p779]{rl}


\subsubsection{Reward Function}

We created a reward function that takes into consideration all the factors to grade the performance of the agent. The reward function gives positive points for keeping the highest number of units alive, negative points for loosing health, positive points for both killing or damaging the enemies, and a negative reward for every time frame in the game that goes through. This way we ensure that the agent wants to attack the enemy while protecting its units; but not prioritizing protecting the units. We control that the agent chooses attacking over hiding or running away by making the reward proportional to how short the match is. \\ 

$R(s) = C_1 numberOfUnits  -  C_2 healthLost  +   C_3 damageDealt  +   C_4 numberOfKills -  C_5 time$ \\ 

We ensure that the reward complies with the convergence restriction of the Q-learning theorem (REFERENCE!!!!). 

%\textit{}\ref{}