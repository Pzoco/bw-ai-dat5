\newpage
\section{$ \hat{Q}$-Learning}\label{section:reinforcement}
In this section we will address the implementation of the Reinforcement Learning Approximation. First we will explain the $ReinforcementLearning$ class and the methods that calculate the value of the $\hat{Q}$ function and updating rules. Then we go through all the necessary calculations made throughout the game to create the data needed in order to use this class and save the relevant information for testing.\\

The $ReinforcementLearning$ class contains all the variables and processes necessary to calculate all the formulas of the $\hat{Q} learning$. It is used throughout the game to update each coefficient of the $\hat{Q}_f$ function, calculate the reward for each state-action pair and saving all the necessary data to continue the calculations in the next iteration of the game (also for saving a data log with all the numbers for further analysis and testing). Further detail of the class implementation can be found in the appendix section \ref{section:RLFS}. 

\subsection{$\hat{Q}_f$ Decision} \label{rlMethods}

Throughout the game, the squad management uses the $\hat{Q}_f$ method \\
$GetBestPositionBasedOnQPotential$ in order to select the next action. It starts with saving the value off all the derivatives ($\frac{\partial \hat{Q}_f(a,s)}{\partial f_i}$). This part is explained in detail in the implementation for every one of the Potential fields. \\

\begin{Sourcecode}[caption=Ally units]
_parameters.dua = MathHelper::GetDistanceToNearestAlly(pos,_unit->getID());
int useAlly = 0;
if(_parameters.da < _variables.ALLYDISTANCE_CONSTANT)
	useAlly = 1;

ally += (double)_variables.FORCEALLY*(double)(2*_parameters.da - _parameters.dua)*useAlly;
(*@\lnote@*)allyQ += (double)(2*_parameters.da - _parameters.dua)*useAlly;
\end{Sourcecode}	

Every time a Potential field is calculated we save the values that correspond to the derivative. In the code shown before for the Ally Units Potential the derivative corresponds to the value of $allyQ$ in \lnnum{1}. We do the same thing to calculate the rest of the derivatives ($edgeQ$, $squadQ$, $coolQ$, $maxdistQ$). Finally just saving them for the future update calculations.\\ 

\begin{Sourcecode}[caption=Saving the values of each derivative]
		StarcraftAI::reinforcementLearning.WriteLiveValue(allyQ);
		StarcraftAI::reinforcementLearning.WriteLiveValue(squadQ); 
		StarcraftAI::reinforcementLearning.WriteLiveValue(maxdistQ);
		StarcraftAI::reinforcementLearning.WriteLiveValue(coolQ);
		StarcraftAI::reinforcementLearning.WriteLiveValue(edgeQ);
\end{Sourcecode}

After saving all the derivatives values for the updating rules, continues with the calculations for the $\hat{Q}_f(a,s)$ as shown before in the Potential fields section \ref{section:IMF}. Finally it calculates the value of $max\hat{Q}_f(a',s'))$. The code used for this calculation follows the same pattern explained for the $\hat{Q}_f(a,s)$ function, the only difference being that the center point the previously calculated $\hat{Q}_f(a,s)$.

\subsection{Reward Function}
 
\textit{CalculateReward Method}: This method applies the formula explained in section \ref{reward} for calculating a reward. This method is called through several frames in the game to calculate the reward value needed for the update function methods.\\

\begin{Sourcecode}[caption=CalculateReward Method]
double ReinforcementLearning::CalculateReward(std::set<BWAPI::Unit*> squad)
{
(*@\lnote@*)//_________________________________________________________	
	double reward = 0.0;
	double maxEnemieHealth = startingEnemies * startingEnemyMaxHealth;
	double maxUnitHealth = startingUnits * startingUnitMaxHealth;
	double enemyCurrentHealth = 0.0;
	double currentUnitHealth = 0.0;
	double numberOfEnemies = 0.0;
	double squadSize = 0.0;
(*@\lnote@*)//_________________________________________________________
	std::set<BWAPI::Player*> enemies = BWAPI::Broodwar->enemies();	
	std::set<BWAPI::Unit*> enemieUnits;

	for(std::set<BWAPI::Player*>::const_iterator i = enemies.begin(); i != enemies.end(); i++)
	{
		std::set<BWAPI::Unit*> tempUnits = (*i)->getUnits();
		for(std::set<BWAPI::Unit*>::iterator j = tempUnits.begin(); j != tempUnits.end(); j++)
		{
			enemieUnits.insert((*j));
			numberOfEnemies++;
		}
	}
(*@\lnote@*)//_________________________________________________________
	for(std::set<BWAPI::Unit*>::iterator j = squad.begin(); j != squad.end(); j++)
	{
		if((*j)->exists()) squadSize++;
		currentUnitHealth += (double)(*j)->getHitPoints();
	}
(*@\lnote@*)//_________________________________________________________
	for(std::set<BWAPI::Unit*>::iterator j = enemieUnits.begin(); j != enemieUnits.end(); j++)
	{
		enemyCurrentHealth += (double)(*j)->getHitPoints();
	}	
//_________________________________________________________
	(*@\lnote@*)reward = c1 * (startingUnits - squadSize) + c2 * (maxUnitHealth-currentUnitHealth) + c3 * (maxEnemieHealth-enemyCurrentHealth) + c4*(startingEnemies-numberOfEnemies)+ c5*(BWAPI::Broodwar->getFrameCount());
	
	(*@\lnote@*)return reward/1000;
}
\end{Sourcecode}
This method begins with the initialization of all the values needed for the reward formula (contained in section \lnnum{1}). The values use some of the already indicated constants in the class fields (view the Field Summary here \ref{app:field}). \\

In section \lnnum{2}, we obtain the array of all the enemy players ($enemies$) and create an array of saving all the enemy units ($enemyUnits$). Then we iterate through the enemy players and save each of their units in the $enemyUnits$ array also counting the $numberOfEnemies$. \\

In section \lnnum{3}, we iterate through the $squad$ (received as a parameter) to count the $squadSize$ and accumulate the total $currentUnitHealth$. Similarly, in section \lnnum{4}, we iterate through the $enemieUnits$ to accumulate the total $currentEnemyHealth$.\\

Then we use all the previous calculated values to evaluate the reward function in \lnnum{4}. Note that we already indicated in the Field Summary which constant value we used for each coefficient. We choose those specific values because they seem to reflect a reward/punishment value correspondent to what we want our agent to learn. Finally, also note that we use this reward value scaled, or reduced (as observed in \lnnum{5}), to impact the rate of change of the updating formulas and reduce it slightly. \\

\subsection{Updating Rules}

\textit{CalculateTheta Method}: This method simply applies the formula explained before in $\hat{Q}$ learning (see reference \ref{rleq}). It corresponds to the updating rule for Q-learning function approximation considering temporal difference.\\

\begin{Sourcecode}[caption=CalculateTheta Method]
double ReinforcementLearning::CalculateTheta(double theta, double reward,double currQ, double nextQ, double derivative)
{
	(*@\lnote@*)double newtheta = theta + alpha * (reward + gamma * nextQ - currQ) * derivative;
	return newtheta;
}
\end{Sourcecode}

In section \ref{rleq_update} there are several different formulas for updating each coefficient in the $\hat{Q}_f$ function, but that distinction between updating rules is external to the $ReinforcementLearning$ class. Therefore, the formula directly translated in \lnnum{1} is the general updating rule. The specific $\hat{Q}$ value, current coefficient and the rest of the parameters are calculated separately because we need them in every frame throughout the game. \\

\subsubsection{Calling the Updating Rules}

We choose to calculate new weights or coefficients for the $\hat{Q}_f$ function every $n$ frames. Trying to update for every single one created a lag in the performance of the game. The optimization we created for updating while still preserving the game speed is the following: 
\pagebreak
\begin{Sourcecode}[caption=Calculations for the Updating Function Values]
(*@\lnote@*)if(BWAPI::Broodwar->getFrameCount() % 25 == 0){
		(*@\lnote@*)\\____________________________________________
		double* liveBufferPointer = StarcraftAI::reinforcementLearning.GetLiveBuffer(); 
		int liveCount = StarcraftAI::reinforcementLearning.GetLiveCount(); 
		StarcraftAI::reinforcementLearning.ClearLiveBuffer(); 
		\\____________________________________________
		double edge, cool, mde, squad, ally, currQ, nextQ, reward; 
		
		(*@\lnote@*)for(int i=0; i<liveCount; i++){
			switch(i%8)
				{
				case 0:
					ally = liveBufferPointer[i];
					break; 
				case 1:
					squad = liveBufferPointer[i];
					break; 
				case 2:
					mde = liveBufferPointer[i];;
					break;
				case 3:
					cool = liveBufferPointer[i];
					break; 
				case 4:
					edge = liveBufferPointer[i];
					break; 
				case 5:
					currQ = liveBufferPointer[i];
					break; 
				case 6:
					nextQ = liveBufferPointer[i];
					break; 
				case 7:
					reward = liveBufferPointer[i];
					(*@\lnote@*)\\______________________________________
					_thetas.edge = ReinforcementLearning::CalculateTheta(_thetas.edge,reward,currQ,nextQ,edge);
					_thetas.cool = ReinforcementLearning::CalculateTheta(_thetas.cool,reward,currQ,nextQ,cool);
					_thetas.mde = ReinforcementLearning::CalculateTheta(_thetas.mde,reward,currQ,nextQ,mde);
					_thetas.squad = ReinforcementLearning::CalculateTheta(_thetas.squad,reward,currQ,nextQ,squad);
					_thetas.ally = ReinforcementLearning::CalculateTheta(_thetas.ally,reward,currQ,nextQ,ally);
					\\______________________________________
					break;
				default:
					break; 
			}
		}
	}
\end{Sourcecode}
\pagebreak
In the first place, we make these calculations every 25 frames as shown in \lnnum{1}, trying to simulate updating once a second. Then we use some of the methods in the $ReinforcementLearning$ class (indicated in section \lnnum{2}) to call all the values that were previously saved in the data buffer. This values includes all the above explained derivatives and $\hat{Q}_f$ values. We separate the values, as shown in \lnnum{3} and then save them in variables for parameter passing to the updating function. Finally in section \lnnum{4} we use the $ReinforcementLearning::CalculateTheta$ method that corresponds to each of the weights and coefficients of the new $\hat{Q}_f$. 
